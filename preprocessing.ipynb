{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Politician Sentiment Analysis - Preprocessing\n",
    "\n",
    "Name:       Devin Patel  \n",
    "Class:      CS 588 - 01  \n",
    "Term:       FA 22  \n",
    "Project:    Determining party alignment based on 2016 Election tweets.  \n",
    "File Purpose: To pull tweets and generate sensitivity and polarity values,\n",
    "              then export all data to a pickle file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tweepy                   # Twitter API\n",
    "import json                     # Twitter API config\n",
    "from textblob import TextBlob   # Sentiment Analysis\n",
    "import re                       # Cleaning Tweets\n",
    "import pandas as pd             # Organizing data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def authenticateTweepy(config_file_path):\n",
    "    \"\"\"\n",
    "    Authenticates Tweepy API\n",
    "\n",
    "    Returns:\n",
    "        tweepy.api.API: API object\n",
    "    \"\"\"\n",
    "    config = json.load(open(config_file_path, 'r'))\n",
    "    authenticate = tweepy.OAuthHandler(config['KEY'], config['SECRET'])\n",
    "    authenticate.set_access_token(config['TOKEN'], config['TOKENSECRET'])\n",
    "    return tweepy.API(authenticate, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "api = authenticateTweepy(r\"auth/config.json\")\n",
    "RANDOM_STATE = 12  # Initialization for methods needing a random_state value\n",
    "TWEETS_AMOUNT = 20000 # Number of tweets to fetch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JOB STATUS] 'Data/democratic-candidate-timelines.txt': 20000/20000\n",
      "Completed Democrat Candidate Statuses\n",
      "\tStatuses Pulled: 20000\n",
      "\tFailed Statuses: 0\n",
      "\n",
      "[JOB STATUS] 'Data/democratic-party-timelines.txt': 10550/20000\n",
      "Completed Democrat Party Statuses\n",
      "\tStatuses Pulled: 10550\n",
      "\tFailed Statuses: 0\n",
      "\n",
      "[JOB STATUS] 'Data/republican-candidate-timelines.txt': 12182/20000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JOB STATUS] 'Data/republican-candidate-timelines.txt': 20000/20000\n",
      "Completed Republican Candidate Statuses\n",
      "\tStatuses Pulled: 20000\n",
      "\tFailed Statuses: 0\n",
      "\n",
      "[JOB STATUS] 'Data/republican-party-timelines.txt': 15402/20000\n",
      "Completed Republican Party Statuses\n",
      "\tStatuses Pulled: 15402\n",
      "\tFailed Statuses: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load status IDs from text files and import tweets\n",
    "def importStatuses(dataPath, num_tweets):\n",
    "    statuses = []\n",
    "    failed_statuses = []\n",
    "    with open(dataPath, 'r') as r:\n",
    "        all_candidate_ids = [i.strip() for i in r.readlines()]\n",
    "        listOfCandidateIds = [all_candidate_ids[i*100: (i+1)*100] for i in range((len(all_candidate_ids)+100-1) // 100)]\n",
    "        i=0\n",
    "        candidate_index = 0\n",
    "        for candidate_ids in listOfCandidateIds:\n",
    "            if i == num_tweets: break\n",
    "            try:\n",
    "                searched_statuses = api.lookup_statuses(id=list(candidate_ids), tweet_mode='extended')\n",
    "                for status in searched_statuses:\n",
    "                    if i == num_tweets: break\n",
    "                    if status.full_text[:3] == \"RT \": continue # Skip tweet if it is a retweet\n",
    "                    statuses.append(status)\n",
    "                    i+=1\n",
    "            except Exception as e:\n",
    "                failed_statuses.append(f\"[FAIL] id='{candidate_index}':  {e}\")\n",
    "                \n",
    "            print(f\"[JOB STATUS] '{dataPath}': {i}/{num_tweets}\", end='\\r')\n",
    "            candidate_index += 1\n",
    "    print()\n",
    "    return statuses, failed_statuses\n",
    "\n",
    "# Loops through to collect data\n",
    "def collectStatusData(statuses, party):\n",
    "    text_data = []\n",
    "    favorite_data = []\n",
    "    retweet_data = []\n",
    "    party_data = []\n",
    "    \n",
    "    for tweet in statuses:\n",
    "        text_data.append(tweet.full_text)\n",
    "        favorite_data.append(tweet.favorite_count)\n",
    "        retweet_data.append(tweet.retweet_count)\n",
    "        party_data.append(party) # 0 = Republican, 1 = Democrat\n",
    "        \n",
    "    return text_data, favorite_data, retweet_data, party_data\n",
    "\n",
    "\n",
    "# Start import code here\n",
    "\n",
    "# Paths\n",
    "export_data_path = r'Data/TweetData.pkl'\n",
    "import_errors_path = r\"Logs/import-errors.txt\"\n",
    "\n",
    "failed_statuses = []\n",
    "\n",
    "# Grabs democrat 2016 candidate tweets\n",
    "democrat_candidate_statuses, r_fail_statuses = importStatuses(r\"Data/democratic-candidate-timelines.txt\", TWEETS_AMOUNT)\n",
    "failed_statuses += r_fail_statuses\n",
    "print(f\"Completed Democrat Candidate Statuses\\n\\tStatuses Pulled: {len(democrat_candidate_statuses)}\\n\\tFailed Statuses: {len(r_fail_statuses)}\\n\")\n",
    "\n",
    "# Grabs democrat 2016 party tweets\n",
    "democrat_party_statuses, r_fail_statuses = importStatuses(r\"Data/democratic-party-timelines.txt\", TWEETS_AMOUNT)\n",
    "failed_statuses += r_fail_statuses\n",
    "print(f\"Completed Democrat Party Statuses\\n\\tStatuses Pulled: {len(democrat_party_statuses)}\\n\\tFailed Statuses: {len(r_fail_statuses)}\\n\")\n",
    "\n",
    "# Grabs republican 2016 candidate tweets\n",
    "republican_candidate_statuses, r_fail_statuses = importStatuses(r\"Data/republican-candidate-timelines.txt\", TWEETS_AMOUNT)\n",
    "failed_statuses += r_fail_statuses\n",
    "print(f\"Completed Republican Candidate Statuses\\n\\tStatuses Pulled: {len(republican_candidate_statuses)}\\n\\tFailed Statuses: {len(r_fail_statuses)}\\n\")\n",
    "\n",
    "# Grabs republican 2016 party tweets\n",
    "republican_party_statuses, r_fail_statuses = importStatuses(r\"Data/republican-party-timelines.txt\", TWEETS_AMOUNT)\n",
    "failed_statuses += r_fail_statuses\n",
    "print(f\"Completed Republican Party Statuses\\n\\tStatuses Pulled: {len(republican_party_statuses)}\\n\\tFailed Statuses: {len(r_fail_statuses)}\\n\")\n",
    "\n",
    "# Loop through and collect data\n",
    "text_data = []\n",
    "favorite_data = []\n",
    "retweet_data = []\n",
    "party_data = []\n",
    "r_data = []\n",
    "\n",
    "r_data.append(collectStatusData(democrat_candidate_statuses, party=1))\n",
    "r_data.append(collectStatusData(democrat_party_statuses, party=1))\n",
    "r_data.append(collectStatusData(republican_candidate_statuses, party=0))\n",
    "r_data.append(collectStatusData(republican_party_statuses, party=0))\n",
    "\n",
    "for tup in r_data:\n",
    "    text_data += tup[0]\n",
    "    favorite_data += tup[1]\n",
    "    retweet_data += tup[2]\n",
    "    party_data += tup[3]\n",
    "\n",
    "data = {'Likes': favorite_data,\n",
    "        'Retweets': retweet_data,\n",
    "        'Party': party_data }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Likes'] = df['Likes'].astype('int')\n",
    "df['Retweets'] = df['Retweets'].astype('int')\n",
    "df['Party'] = df['Party'].astype('int')\n",
    "\n",
    "text_df = pd.DataFrame(text_data, columns=['Text'])\n",
    "\n",
    "# Exports errors\n",
    "with open(import_errors_path, 'w') as w:\n",
    "    for status in failed_statuses:\n",
    "        w.write(status+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text of irreleveant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def cleanText(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing @mentions, hashtag symbols, and hyperlinks.\n",
    "\n",
    "    Args:\n",
    "        text (string): Text to clean\n",
    "\n",
    "    Returns:\n",
    "        string: Cleaned text\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", '', text)   # Removes @mentions\n",
    "    text = re.sub(r\"#\", '', text)               # Removes hashtag symbol\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", '', text)  # Removes hyperlinks\n",
    "    \n",
    "    return text\n",
    "\n",
    "text_df['Text'] = text_df['Text'].apply(cleanText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subjectivity and polarity\n",
    "def getSubjectivity(text):\n",
    "    \"\"\"\n",
    "    Generates subjectivity value.\n",
    "    Subjectivity - how 'opinionated' some text is\n",
    "\n",
    "    Args:\n",
    "        text (string): Text to assess\n",
    "\n",
    "    Returns:\n",
    "        float: subjectivity\n",
    "    \"\"\"\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(text):\n",
    "    \"\"\"\n",
    "    Generates polarity value.\n",
    "    Polarity - how 'positive' or 'negative' some text is\n",
    "\n",
    "    Args:\n",
    "        text (string): Text to assess\n",
    "\n",
    "    Returns:\n",
    "        float: polarity\n",
    "    \"\"\"\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "df['Subjectivity'] = text_df['Text'].apply(getSubjectivity).astype('float')\n",
    "df['Polarity'] = text_df['Text'].apply(getPolarity).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Likes  Retweets  Party  Subjectivity  Polarity\n",
      "0       6049      2548      1      0.000000  0.000000\n",
      "1       1264       594      1      0.766667  0.366667\n",
      "2       7673      3507      1      0.600000  0.875000\n",
      "3       4663      1480      1      0.500000  0.062500\n",
      "4       1836       748      1      0.000000  0.000000\n",
      "...      ...       ...    ...           ...       ...\n",
      "65947    601       325      0      0.000000  0.000000\n",
      "65948     23         9      0      0.300000  0.000000\n",
      "65949    247       234      0      0.252083  0.125000\n",
      "65950     13         8      0      0.000000  0.000000\n",
      "65951     35        10      0      0.375000 -0.125000\n",
      "\n",
      "[65952 rows x 5 columns]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Data is finished cleaning, export to pkl file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(df)\n\u001b[1;32m----> 3\u001b[0m pd\u001b[39m.\u001b[39;49mto_pickle(df, \u001b[39mopen\u001b[39;49m(export_data_path, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\xdp20\\OneDrive\\Assignments and Projects\\CS 588 - Intro to Big Data Computing\\Term Project\\twitter-political-party-classifier\\TermProjectEnv\\lib\\site-packages\\pandas\\io\\pickle.py:112\u001b[0m, in \u001b[0;36mto_pickle\u001b[1;34m(obj, filepath_or_buffer, compression, protocol, storage_options)\u001b[0m\n\u001b[0;32m    109\u001b[0m     handles\u001b[39m.\u001b[39mhandle\u001b[39m.\u001b[39mwrite(pickle\u001b[39m.\u001b[39mdumps(obj, protocol\u001b[39m=\u001b[39mprotocol))\n\u001b[0;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[39m# letting pickle write directly to the buffer is more memory-efficient\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(obj, handles\u001b[39m.\u001b[39;49mhandle, protocol\u001b[39m=\u001b[39;49mprotocol)\n",
      "\u001b[1;31mTypeError\u001b[0m: write() argument must be str, not bytes"
     ]
    }
   ],
   "source": [
    "# Data is finished cleaning, export to pkl file\n",
    "pd.to_pickle(df, open(export_data_path, 'w'))\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('TermProjectEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46e72866615fe58db29ecb495c2e0a5a245101998666c55885bf45c1dc41d765"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
