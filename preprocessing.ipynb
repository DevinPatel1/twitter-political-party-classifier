{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Politician Sentiment Analysis - Data Gathering\n",
    "\n",
    "Name:       Devin Patel  \n",
    "Class:      CS 588 - 01  \n",
    "Term:       FA 22  \n",
    "Project:    Determining party alignment based on 2016 Election tweets.  \n",
    "File Purpose: To pull tweets and generate sensitivity and polarity values,\n",
    "              then export all data to pickle and csv files.  \n",
    "\n",
    "\n",
    "Tweets are collected based on status IDs compiled by Justin Littman, Laura Wrubel, and Daniel Kerchner\n",
    "on the [Harvard Dataverse.](dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/PDI7IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tweepy                   # Twitter API\n",
    "import json                     # Twitter API config\n",
    "from textblob import TextBlob   # Sentiment Analysis\n",
    "import re                       # Cleaning Tweets\n",
    "import pandas as pd             # Organizing data\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "\n",
    "def authenticateTweepy(config_file_path):\n",
    "    \"\"\"\n",
    "    Authenticates Tweepy API\n",
    "\n",
    "    Returns:\n",
    "        tweepy.api.API: API object\n",
    "    \"\"\"\n",
    "    config = json.load(open(config_file_path, 'r'))\n",
    "    authenticate = tweepy.OAuthHandler(config['KEY'], config['SECRET'])\n",
    "    authenticate.set_access_token(config['TOKEN'], config['TOKENSECRET'])\n",
    "    return tweepy.API(authenticate, wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "api = authenticateTweepy(r\"auth/config.json\")\n",
    "RANDOM_STATE = 12  # Initialization for methods needing a random_state value\n",
    "TWEETS_AMOUNT = 20000 # Number of tweets to fetch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JOB STATUS] 'democratic-candidate-timelines.txt': 20000/20000\n",
      "Completed Democrat Candidate Statuses\n",
      "\tStatuses Pulled: 20000\n",
      "\tFailed Statuses: 0\n",
      "\n",
      "[JOB STATUS] 'democratic-party-timelines.txt': 10550/20000\n",
      "Completed Democrat Party Statuses\n",
      "\tStatuses Pulled: 10550\n",
      "\tFailed Statuses: 0\n",
      "\n",
      "[JOB STATUS] 'republican-candidate-timelines.txt': 20000/20000\n",
      "Completed Republican Candidate Statuses\n",
      "\tStatuses Pulled: 20000\n",
      "\tFailed Statuses: 0\n",
      "\n",
      "[JOB STATUS] 'republican-party-timelines.txt': 10993/20000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JOB STATUS] 'republican-party-timelines.txt': 15402/20000\n",
      "Completed Republican Party Statuses\n",
      "\tStatuses Pulled: 15402\n",
      "\tFailed Statuses: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load status IDs from text files and import tweets\n",
    "def pullStatuses(dataPath, num_tweets):\n",
    "    # Tweepy.API.lookup_statuses() only allows up to 100 IDs to be queried,\n",
    "    # so all the IDs must be broken up into 100-element lists.\n",
    "    \n",
    "    statuses = []\n",
    "    failed_statuses = []\n",
    "    with open(dataPath, 'r') as r:\n",
    "        all_candidate_ids = [i.strip() for i in r.readlines()]\n",
    "        listOfCandidateIds = [all_candidate_ids[i*100: (i+1)*100] for i in range((len(all_candidate_ids)+100-1) // 100)]\n",
    "        i=0\n",
    "        row_index = 1\n",
    "        for candidate_ids in listOfCandidateIds:\n",
    "            if i == num_tweets: break\n",
    "            try:\n",
    "                searched_statuses = api.lookup_statuses(id=list(candidate_ids), tweet_mode='extended')\n",
    "                for status in searched_statuses:\n",
    "                    row_index += 1\n",
    "                    if i == num_tweets: break\n",
    "                    if status.full_text[:3] == \"RT \": continue # Skip tweet if it is a retweet\n",
    "                    statuses.append(status)\n",
    "                    i+=1\n",
    "\n",
    "            except Exception as e:\n",
    "                failed_statuses.append(f\"[FAIL] File '{os.path.basename(dataPath)}'  Row {row_index}:  {e}\")\n",
    "                \n",
    "            print(f\"[JOB STATUS] '{os.path.basename(dataPath)}': {i}/{num_tweets}\", end='\\r')\n",
    "    print()\n",
    "    return statuses, failed_statuses\n",
    "\n",
    "# Loops through to collect data\n",
    "def collectStatusData(statuses, party):\n",
    "    text_data = []\n",
    "    favorite_data = []\n",
    "    retweet_data = []\n",
    "    party_data = []\n",
    "    \n",
    "    for tweet in statuses:\n",
    "        text_data.append(tweet.full_text)\n",
    "        favorite_data.append(tweet.favorite_count)\n",
    "        retweet_data.append(tweet.retweet_count)\n",
    "        party_data.append(party) # 0 = Republican, 1 = Democrat\n",
    "        \n",
    "    return text_data, favorite_data, retweet_data, party_data\n",
    "\n",
    "\n",
    "# Start import code here\n",
    "\n",
    "# Paths\n",
    "export_data_path = r'Data/TweetData.pkl'\n",
    "import_errors_path = r\"Logs/import-errors.txt\"\n",
    "\n",
    "failed_statuses = []\n",
    "\n",
    "# Grabs democrat 2016 candidate tweets\n",
    "democrat_candidate_statuses, r_fail_statuses = pullStatuses(r\"Data/democratic-candidate-timelines.txt\", TWEETS_AMOUNT)\n",
    "failed_statuses += r_fail_statuses\n",
    "print(f\"Completed Democrat Candidate Statuses\\n\\tStatuses Pulled: {len(democrat_candidate_statuses)}\\n\\tFailed Statuses: {len(r_fail_statuses)}\\n\")\n",
    "\n",
    "# Grabs democrat 2016 party tweets\n",
    "democrat_party_statuses, r_fail_statuses = pullStatuses(r\"Data/democratic-party-timelines.txt\", TWEETS_AMOUNT)\n",
    "failed_statuses += r_fail_statuses\n",
    "print(f\"Completed Democrat Party Statuses\\n\\tStatuses Pulled: {len(democrat_party_statuses)}\\n\\tFailed Statuses: {len(r_fail_statuses)}\\n\")\n",
    "\n",
    "# Grabs republican 2016 candidate tweets\n",
    "republican_candidate_statuses, r_fail_statuses = pullStatuses(r\"Data/republican-candidate-timelines.txt\", TWEETS_AMOUNT)\n",
    "failed_statuses += r_fail_statuses\n",
    "print(f\"Completed Republican Candidate Statuses\\n\\tStatuses Pulled: {len(republican_candidate_statuses)}\\n\\tFailed Statuses: {len(r_fail_statuses)}\\n\")\n",
    "\n",
    "# Grabs republican 2016 party tweets\n",
    "republican_party_statuses, r_fail_statuses = pullStatuses(r\"Data/republican-party-timelines.txt\", TWEETS_AMOUNT)\n",
    "failed_statuses += r_fail_statuses\n",
    "print(f\"Completed Republican Party Statuses\\n\\tStatuses Pulled: {len(republican_party_statuses)}\\n\\tFailed Statuses: {len(r_fail_statuses)}\\n\")\n",
    "\n",
    "# Loop through and collect data\n",
    "text_data = []\n",
    "favorite_data = []\n",
    "retweet_data = []\n",
    "party_data = []\n",
    "r_data = []\n",
    "\n",
    "r_data.append(collectStatusData(democrat_candidate_statuses, party=1))\n",
    "r_data.append(collectStatusData(democrat_party_statuses, party=1))\n",
    "r_data.append(collectStatusData(republican_candidate_statuses, party=0))\n",
    "r_data.append(collectStatusData(republican_party_statuses, party=0))\n",
    "\n",
    "for tup in r_data:\n",
    "    text_data += tup[0]\n",
    "    favorite_data += tup[1]\n",
    "    retweet_data += tup[2]\n",
    "    party_data += tup[3]\n",
    "\n",
    "data = {'Likes': favorite_data,\n",
    "        'Retweets': retweet_data,\n",
    "        'Party': party_data }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Likes'] = df['Likes'].astype('int')\n",
    "df['Retweets'] = df['Retweets'].astype('int')\n",
    "df['Party'] = df['Party'].astype('int')\n",
    "\n",
    "text_df = pd.DataFrame(text_data, columns=['Text'])\n",
    "\n",
    "# Exports errors\n",
    "if failed_statuses:\n",
    "    with open(import_errors_path, 'w') as w:\n",
    "        for status in failed_statuses:\n",
    "            w.write(status+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text of irreleveant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def cleanText(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing @mentions, hashtag symbols, and hyperlinks.\n",
    "\n",
    "    Args:\n",
    "        text (string): Text to clean\n",
    "\n",
    "    Returns:\n",
    "        string: Cleaned text\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"@[A-Za-z0-9]+\", '', text)   # Removes @mentions\n",
    "    text = re.sub(r\"#\", '', text)               # Removes hashtag symbol\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", '', text)  # Removes hyperlinks\n",
    "    \n",
    "    return text\n",
    "\n",
    "text_df['Text'] = text_df['Text'].apply(cleanText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sentiment Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subjectivity and polarity\n",
    "def getSubjectivity(text):\n",
    "    \"\"\"\n",
    "    Generates subjectivity value.\n",
    "    Subjectivity - how 'opinionated' some text is\n",
    "\n",
    "    Args:\n",
    "        text (string): Text to assess\n",
    "\n",
    "    Returns:\n",
    "        float: subjectivity\n",
    "    \"\"\"\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(text):\n",
    "    \"\"\"\n",
    "    Generates polarity value.\n",
    "    Polarity - how 'positive' or 'negative' some text is\n",
    "\n",
    "    Args:\n",
    "        text (string): Text to assess\n",
    "\n",
    "    Returns:\n",
    "        float: polarity\n",
    "    \"\"\"\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "df['Subjectivity'] = text_df['Text'].apply(getSubjectivity).astype('float')\n",
    "df['Polarity'] = text_df['Text'].apply(getPolarity).astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Likes  Retweets  Party  Subjectivity  Polarity\n",
      "0       6031      2544      1      0.000000  0.000000\n",
      "1       1260       594      1      0.766667  0.366667\n",
      "2       7641      3505      1      0.600000  0.875000\n",
      "3       4651      1476      1      0.500000  0.062500\n",
      "4       1827       748      1      0.000000  0.000000\n",
      "...      ...       ...    ...           ...       ...\n",
      "65947    600       325      0      0.000000  0.000000\n",
      "65948     23         9      0      0.300000  0.000000\n",
      "65949    248       233      0      0.252083  0.125000\n",
      "65950     13         8      0      0.000000  0.000000\n",
      "65951     36        10      0      0.375000 -0.125000\n",
      "\n",
      "[65952 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Data is finished cleaning, export to pkl file for quick access\n",
    "df.to_pickle(export_data_path)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV for viewable export\n",
    "from csv import DictWriter\n",
    "\n",
    "def export_dataframe_as_csv(df, outfile):\n",
    "    with open(outfile, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        # Column headers\n",
    "        fieldnames = df.columns.values.tolist() + ['Text']\n",
    "\n",
    "        writer = DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Loops through every row of dataframe\n",
    "        for i in df.index:\n",
    "            row = df.iloc[i]\n",
    "            row_out = {}\n",
    "            \n",
    "            \"\"\"\n",
    "            This step must be done with a for loop because\n",
    "            'row' is of type 'pandas.Series' while\n",
    "            'row_out' is of type 'dict'\n",
    "            \"\"\"\n",
    "            # Loops through all columns to build output row\n",
    "            for feature in fieldnames[:-1]: row_out[feature] = row[feature]\n",
    "            \n",
    "            # Adds text to the end of row columns\n",
    "            row_out['Text'] = text_df.iloc[i]['Text']\n",
    "            \n",
    "            # Writes row to file\n",
    "            writer.writerow(row_out)\n",
    "\n",
    "\n",
    "\n",
    "# File path of csv file\n",
    "outfile = r\"Data/TweetData.csv\"\n",
    "export_dataframe_as_csv(df, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('TermProjectEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46e72866615fe58db29ecb495c2e0a5a245101998666c55885bf45c1dc41d765"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
